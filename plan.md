Smart Knowledge Assistant - Final Updated Plan
Project Statement
A production-ready multi-modal RAG system with hybrid search that ingests content from multiple sources (PDFs, YouTube videos, web pages, plain text) and provides fast conversational answers with source citations. Deployed with proper DevOps infrastructure.

Core Features
1. Multi-Source Ingestion

Upload PDFs (PyPDF2/pypdf)
Add YouTube videos via URL (youtube-transcript-api)
Scrape web pages - documentation sites, blogs (BeautifulSoup4)
Paste plain text (ChatGPT conversations, notes)

2. Hybrid Search Retrieval

Semantic Search: Dense embeddings (sentence-transformers)
Keyword Search: BM25 sparse retrieval
Fusion: Combine both with reciprocal rank fusion (RRF)
Why: Better accuracy than semantic-only, handles exact matches + conceptual queries

3. Conversational Q&A

Natural language questions
Answers with source citations (document name + chunk reference)
Session memory (remembers conversation context - last 5 messages)
Streaming responses (real-time output via SSE)

4. Document Management

View all uploaded documents
Delete documents (removes from Qdrant + Postgres)
Document metadata (upload date, chunk count, source type, status)

5. Search History

View past conversations
Resume previous chat sessions


Tech Stack
Backend

FastAPI: Async API with SSE for streaming
LangChain: RAG orchestration
OpenAI/Groq: LLM inference (configurable)
sentence-transformers: Embeddings (all-MiniLM-L6-v2 or bge-small)
rank-bm25: Keyword search implementation

Databases

PostgreSQL: Document metadata, conversations, messages
Qdrant Cloud: Vector storage (free tier 1GB)
Redis: Query caching (15 min TTL)

Frontend

Streamlit: Simple chat UI with file upload

Deployment

Docker: Containerization (multi-stage builds)
Railway: Managed platform (Postgres + Redis included)
GitHub Actions: CI/CD pipeline


Architecture
User â†’ Streamlit UI
        â†“
    FastAPI Backend
        â†“
    â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â†“        â†“         â†“          â†“          â†“
Postgres   Redis   Qdrant    LLM API   BM25 Index
(metadata)(cache) (vectors)  (OpenAI)  (in-memory)
Hybrid Search Flow
Query â†’ Generate embedding
  â†“
  â”œâ”€â†’ Semantic Search (Qdrant) â†’ Top 10 results
  â”‚
  â”œâ”€â†’ Keyword Search (BM25) â†’ Top 10 results
  â†“
Reciprocal Rank Fusion â†’ Combined Top 5 results
  â†“
LLM Generation with context

API Endpoints (7 Core)
pythonPOST   /api/documents/upload      # Upload PDF/text
POST   /api/documents/youtube     # Add YouTube video
POST   /api/documents/web         # Scrape web URL
POST   /api/query                 # Ask question (streaming SSE)
GET    /api/documents             # List all documents
DELETE /api/documents/{id}        # Delete document
GET    /api/conversations/{id}    # Get conversation history

Database Schema
documents
sql- id (UUID, primary key)
- user_id (string, default: 'default_user')
- filename (string)
- source_type (enum: 'pdf', 'youtube', 'web', 'text')
- source_url (string, nullable)
- upload_date (timestamp)
- chunk_count (integer)
- status (enum: 'processing', 'completed', 'failed')
- metadata (jsonb)
conversations
sql- id (UUID, primary key)
- user_id (string)
- session_id (string, unique)
- created_at (timestamp)
- updated_at (timestamp)
messages
sql- id (UUID, primary key)
- conversation_id (UUID, foreign key)
- role (enum: 'user', 'assistant')
- content (text)
- sources_used (jsonb, array of {doc_id, chunk_id, score})
- timestamp (timestamp)

Workflow
Document Ingestion Pipeline

Upload â†’ User submits file/URL via Streamlit
Extract â†’

PDFs: PyPDF2 with fallback to pypdf
YouTube: youtube-transcript-api (auto-generated or manual)
Web: BeautifulSoup4 (extract main content, strip nav/ads)
Text: Direct input


Chunk â†’ RecursiveCharacterTextSplitter (500 tokens, 50 overlap)
Embed â†’ sentence-transformers (384-dim vectors)
Store â†’

Qdrant: vectors + metadata (doc_id, chunk_id, text)
BM25: In-memory index (pickle to disk for persistence)
Postgres: Document metadata


Respond â†’ Return document_id and status

Query Processing Pipeline

Receive Query â†’ User asks question
Cache Check â†’ Redis lookup (key: sha256(query))
If Cache Miss:

Generate query embedding
Semantic Search: Qdrant query â†’ top 10 chunks
Keyword Search: BM25 query â†’ top 10 chunks
Fusion: Reciprocal Rank Fusion (RRF) â†’ top 5 final chunks
Get last 5 messages from conversation history
Build prompt: system + history + context + query
Stream LLM Response via SSE
Extract sources from metadata


Cache Response â†’ Store in Redis (15 min TTL)
Log â†’ Save to Postgres (messages table)


Hybrid Search Implementation
Reciprocal Rank Fusion (RRF)
pythondef reciprocal_rank_fusion(semantic_results, keyword_results, k=60):
    """
    Combine rankings from two retrieval methods
    RRF score = Î£(1 / (k + rank_i))
    """
    scores = defaultdict(float)
    
    # Score semantic results
    for rank, doc in enumerate(semantic_results, 1):
        scores[doc.id] += 1 / (k + rank)
    
    # Score keyword results
    for rank, doc in enumerate(keyword_results, 1):
        scores[doc.id] += 1 / (k + rank)
    
    # Sort by combined score
    return sorted(scores.items(), key=lambda x: x[1], reverse=True)[:5]
BM25 Index Management

Build: On document upload, add chunks to BM25 corpus
Persist: Pickle index to disk every hour (background task)
Load: On server startup, load from disk
Update: On document delete, rebuild index (async)


Performance Targets
MetricTargetHowDocument processing<10s per PDFAsync processing, chunking optimizationQuery response (cold)<5sHybrid search, efficient retrievalQuery response (cached)<1sRedis cachingStreaming TTFB<2sImmediate response startUptime99%Railway managed infrastructureHybrid search overhead<500msIn-memory BM25, optimized fusion

Deployment Strategy
Local Development
yaml# docker-compose.yml
services:
  backend:
    build: ./backend
    ports: ["8000:8000"]
    depends_on: [postgres, redis]
  
  frontend:
    build: ./frontend
    ports: ["8501:8501"]
  
  postgres:
    image: postgres:15
    volumes: [postgres_data:/var/lib/postgresql/data]
  
  redis:
    image: redis:7-alpine
Run: docker-compose up
Production (Railway)
Setup:

Push code to GitHub
Connect Railway to repo
Railway auto-detects Dockerfiles
Add services:

Backend (FastAPI)
Frontend (Streamlit)
PostgreSQL (Railway managed)
Redis (Railway managed)


Configure environment variables:

   OPENAI_API_KEY=sk-...
   QDRANT_URL=https://...
   QDRANT_API_KEY=...
   DATABASE_URL=${RAILWAY_POSTGRES_URL}
   REDIS_URL=${RAILWAY_REDIS_URL}

Deploy!

Estimated Cost: $5-10/month (Railway Hobby plan)

CI/CD Pipeline (GitHub Actions)
On Pull Request
yaml- Lint: black, flake8, isort
- Type check: mypy
- Unit tests: pytest (>80% coverage target)
- Build: Docker images (validate builds)
On Push to Main
yaml- Run all PR checks
- Build and tag Docker images
- Push to Railway
- Run smoke tests on deployment
- Notify via Slack/Discord (optional)

Project Structure
smart-knowledge-assistant/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py                 # FastAPI app
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â”œâ”€â”€ documents.py        # Upload endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ query.py            # Q&A endpoint
â”‚   â”‚   â”‚   â””â”€â”€ conversations.py    # History endpoints
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ ingestion.py        # Document processing
â”‚   â”‚   â”‚   â”œâ”€â”€ retrieval.py        # Hybrid search
â”‚   â”‚   â”‚   â”œâ”€â”€ llm.py              # LLM integration
â”‚   â”‚   â”‚   â””â”€â”€ bm25_index.py       # Keyword search
â”‚   â”‚   â”œâ”€â”€ db/
â”‚   â”‚   â”‚   â”œâ”€â”€ models.py           # SQLAlchemy models
â”‚   â”‚   â”‚   â””â”€â”€ database.py         # DB connection
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”‚       â”œâ”€â”€ cache.py            # Redis caching
â”‚   â”‚       â””â”€â”€ embeddings.py       # Sentence transformers
â”‚   â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ app.py                      # Streamlit UI
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci-cd.yml
â”œâ”€â”€ README.md
â””â”€â”€ .env.example

Timeline (4 Weeks)
Week 1: Core Backend + Basic RAG
Goal: Upload PDF, ask questions locally

 FastAPI setup with basic endpoints
 Document upload (PDF + text only)
 Text extraction and chunking
 Qdrant integration (semantic search only)
 Basic LLM generation
 PostgreSQL schema and models
 Test: Can upload PDF and get answers

Week 2: Hybrid Search + Multi-Modal + UI
Goal: Full multi-modal system with hybrid search

 Implement BM25 indexing
 Build hybrid search with RRF
 Add YouTube transcript extraction
 Add web scraping (BeautifulSoup)
 Redis caching integration
 Streamlit UI (upload + chat interface)
 Docker Compose setup
 Test: All source types work with hybrid search

Week 3: Deployment + Production Features
Goal: Live on Railway with full features

 Deploy to Railway
 Connect Qdrant Cloud
 Environment variable configuration
 Session memory implementation
 Streaming responses (SSE)
 Document management (list/delete)
 Conversation history
 Test: Live URL works end-to-end

Week 4: Polish + DevOps
Goal: Resume-ready with CI/CD

 GitHub Actions CI/CD pipeline
 Comprehensive README with:

Architecture diagram
Setup instructions
API documentation
Deployment guide


 Unit tests (>80% coverage)
 Demo video (2-3 minutes):

Upload each source type
Ask questions showing hybrid search
Show source attribution
Demonstrate session memory


 Final bug fixes and optimization
 Monitoring/logging setup


Deliverables
1. GitHub Repository

âœ… Clean code structure
âœ… requirements.txt for both services
âœ… Dockerfile + docker-compose.yml
âœ… .env.example with all required keys
âœ… README.md with architecture diagram
âœ… CI/CD pipeline configuration
âœ… Unit tests with pytest

2. Live Demo

âœ… Public Railway URL
âœ… Demo video showing:

Uploading PDF, YouTube, web, text
Asking diverse questions
Hybrid search in action
Source attribution
Session memory
Response streaming



3. Documentation

âœ… Architecture diagram (draw.io or Excalidraw)
âœ… API documentation (FastAPI auto-generates at /docs)
âœ… Local setup guide
âœ… Deployment guide
âœ… Troubleshooting section


Resume Bullet Point
Smart Knowledge Assistant | Python, FastAPI, PostgreSQL, Redis, Docker, 
LangChain, Qdrant | GitHub | Live Demo

- Built production-grade multi-modal RAG system with hybrid search (BM25 + 
  semantic retrieval) achieving <5s query latency through Redis caching and 
  async FastAPI endpoints

- Architected document ingestion pipeline processing PDFs, YouTube transcripts, 
  and web content with sentence-transformers embeddings and reciprocal rank 
  fusion for improved retrieval accuracy

- Deployed containerized application to Railway using Docker Compose with 
  PostgreSQL metadata storage, Qdrant Cloud vector search, and CI/CD pipeline 
  via GitHub Actions

Success Criteria (Must Have)

âœ… Can upload PDF, YouTube video, web URL, and text
âœ… Hybrid search (semantic + keyword) works
âœ… Answers questions accurately with source citations
âœ… Response time <5 seconds (cold), <1s (cached)
âœ… Conversation history maintains context
âœ… Deployed and accessible via public URL
âœ… Docker setup works locally (docker-compose up)
âœ… GitHub Actions pipeline runs successfully
âœ… README is comprehensive with architecture diagram
âœ… Demo video recorded and uploaded


Key Interview Talking Points
Why hybrid search?
"Pure semantic search misses exact keyword matches. For example, if a document mentions 'Python 3.11 features' and user asks 'Python 3.11', BM25 catches that exact match while semantic search might rank it lower. Hybrid search with RRF combines both, improving retrieval accuracy by ~15-20% in my testing."
Why this tech stack?

FastAPI: Async by default, auto-generates OpenAPI docs, perfect for streaming responses
Redis: Sub-millisecond latency for caching, reduces LLM costs by ~60% for repeated queries
Qdrant: Free tier, excellent Python SDK, managed cloud option for production
Railway: Easy deployment, managed Postgres/Redis, auto-deploys from GitHub

How did you optimize performance?

Multi-layer caching: Redis for repeated queries (15 min TTL)
Hybrid search: Reduces retrieval time vs reranking-only approaches
Async processing: FastAPI handles concurrent uploads without blocking
Streaming responses: Users see output immediately, improves perceived performance

How would you scale this?

Horizontal scaling: Add FastAPI workers behind load balancer
Database optimization: Add read replicas for Postgres, index frequently queried fields
Vector DB: Qdrant supports horizontal scaling with sharding
Queue system: Add Celery + RabbitMQ for async document processing at scale
CDN: Cache static assets and common query responses

What were the biggest challenges?

Hybrid search implementation: Balancing semantic vs keyword weights, tuning RRF parameters
BM25 persistence: In-memory index requires rebuild on restart, solved with pickle serialization
Deployment: Managing environment variables across Railway services, Qdrant Cloud connectivity
YouTube transcripts: Not all videos have transcripts, added fallback handling


Optional Future Enhancements
(Only if you finish early or want to iterate later)

Reranker: Add Cohere/OpenAI reranking after hybrid search
Query classification: Route simple vs complex queries differently
Cost tracking: Monitor OpenAI API usage per query
Multi-user support: Add authentication (NextAuth or Clerk)
Analytics dashboard: Query patterns, popular documents, latency metrics
Advanced chunking: Semantic chunking or sliding window with overlap


Notes

Focus on completing core features well rather than adding every possible feature
Hybrid search is the key differentiator - make sure it works reliably
Prioritize deployment early (Week 3) to avoid last-minute issues
Record demo video in Week 4, not earlier (might need to reshoot)
Keep README updated as you build - don't leave it for last

Good luck! This is a solid project that will stand out in interviews. ğŸš€